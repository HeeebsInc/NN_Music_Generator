{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from keras.models import load_model\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Music With Neural Network:   0%|                                                                                                                          | 0/50 [00:00<?, ?it/s]\u001b[AINFO:plaidml:Analyzing Ops: 3412 of 8353 operations complete\n",
      "\n",
      "Creating Music With Neural Network:   2%|██▎                                                                                                               | 1/50 [00:14<11:57, 14.64s/it]\u001b[A\n",
      "Creating Music With Neural Network:   8%|█████████                                                                                                         | 4/50 [00:14<07:51, 10.26s/it]\u001b[A\n",
      "Creating Music With Neural Network:  14%|███████████████▉                                                                                                  | 7/50 [00:14<05:09,  7.19s/it]\u001b[A\n",
      "Creating Music With Neural Network:  20%|██████████████████████▌                                                                                          | 10/50 [00:15<03:21,  5.05s/it]\u001b[A\n",
      "Creating Music With Neural Network:  26%|█████████████████████████████▍                                                                                   | 13/50 [00:15<02:11,  3.55s/it]\u001b[A\n",
      "Creating Music With Neural Network:  32%|████████████████████████████████████▏                                                                            | 16/50 [00:15<01:24,  2.49s/it]\u001b[A\n",
      "Creating Music With Neural Network:  38%|██████████████████████████████████████████▉                                                                      | 19/50 [00:15<00:54,  1.76s/it]\u001b[A\n",
      "Creating Music With Neural Network:  44%|█████████████████████████████████████████████████▋                                                               | 22/50 [00:15<00:34,  1.24s/it]\u001b[A\n",
      "Creating Music With Neural Network:  50%|████████████████████████████████████████████████████████▌                                                        | 25/50 [00:15<00:22,  1.13it/s]\u001b[A\n",
      "Creating Music With Neural Network:  56%|███████████████████████████████████████████████████████████████▎                                                 | 28/50 [00:15<00:13,  1.59it/s]\u001b[A\n",
      "Creating Music With Neural Network:  62%|██████████████████████████████████████████████████████████████████████                                           | 31/50 [00:15<00:08,  2.20it/s]\u001b[A\n",
      "Creating Music With Neural Network:  68%|████████████████████████████████████████████████████████████████████████████▊                                    | 34/50 [00:16<00:05,  3.02it/s]\u001b[A\n",
      "Creating Music With Neural Network:  74%|███████████████████████████████████████████████████████████████████████████████████▌                             | 37/50 [00:16<00:03,  4.10it/s]\u001b[A\n",
      "Creating Music With Neural Network:  80%|██████████████████████████████████████████████████████████████████████████████████████████▍                      | 40/50 [00:16<00:01,  5.45it/s]\u001b[A\n",
      "Creating Music With Neural Network:  86%|█████████████████████████████████████████████████████████████████████████████████████████████████▏               | 43/50 [00:16<00:00,  7.08it/s]\u001b[A\n",
      "Creating Music With Neural Network:  92%|███████████████████████████████████████████████████████████████████████████████████████████████████████▉         | 46/50 [00:16<00:00,  8.96it/s]\u001b[A\n",
      "Creating Music With Neural Network: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:16<00:00,  2.99it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "#WITHOUT INSTRUMNET\n",
    "def read_ts_midi(file_path, timestep): \n",
    "    notes = [] \n",
    "    notes_to_parse = None \n",
    "\n",
    "\n",
    "    midi= converter.parse(file_path)\n",
    "\n",
    "    s2 = instrument.partitionByInstrument(midi)\n",
    "    for part in s2.parts:\n",
    "        #select elements of only piano\n",
    "#         if 'Piano' in str(part): \n",
    "\n",
    "        notes_to_parse = part.recurse() \n",
    "\n",
    "        #finding whether a particular element is note or a chord\n",
    "        for element in notes_to_parse:\n",
    "\n",
    "            #note\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "\n",
    "            #chord\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "    notes = np.array(notes)\n",
    "#     np.random.shuffle(notes)\n",
    "    return notes[:timestep]\n",
    "\n",
    "        \n",
    "def get_pickles(pick_name): \n",
    "    x_train, x_test, y_train, y_test, notes_dict = pickle.load(open(f'../Pickles/{pick_name}', 'rb'))\n",
    "    return x_train, x_test, y_train, y_test, notes_dict\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_midi(prediction_output, filename):\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                output_notes.append(instrument.Piano())\n",
    "                cn=int(current_note)\n",
    "                new_note = note.Note(cn)\n",
    "#                 new_note.storedInstrument = instrument.Violin()\n",
    "                notes.append(new_note)\n",
    "                \n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "            \n",
    "        else:\n",
    "            output_notes.append(instrument.Piano())\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "#             new_note.storedInstrument = instrument.Violin()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += .5\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp=filename)\n",
    "\n",
    "\n",
    "def generate_music(model, first_sequence, timestep = 100, total_runs = 25):\n",
    "    song = [i for i in first_sequence]\n",
    "    total_iter = 0\n",
    "    current_idx = 0\n",
    "    pbar = tqdm(desc= 'Creating Music With Neural Network', total = total_runs)\n",
    "    while total_iter < total_runs:\n",
    "        current_seq = song[current_idx: timestep+current_idx]\n",
    "        current_seq = np.array(current_seq).reshape(1, timestep, 1)\n",
    "        next_note = np.argmax(model.predict(current_seq)[0])\n",
    "        song.append(next_note)\n",
    "        total_iter += 1 \n",
    "        current_idx += 1\n",
    "        pbar.update(1)\n",
    "    \n",
    "    return song\n",
    "            \n",
    "test_type = 'Tester'\n",
    "x_train, x_test, y_train, y_test, notes_dict = get_pickles(f'{test_type}.p')\n",
    "reverse_notes = {n:idx for idx, n in notes_dict.items()}\n",
    "first_seq = np.random.randint(0, len(reverse_notes), size = 100)\n",
    "# first_seq = [reverse_notes[i] for i in first_seq]\n",
    "model = load_model(f'ModelWeights/LSTM_{test_type}.h5')\n",
    "\n",
    "new_song = generate_music(model, first_seq, total_runs = 50)\n",
    "new_song_notes = [notes_dict[i] for i in new_song]\n",
    "    \n",
    "convert_to_midi(new_song_notes, filename = f'Tests/{test_type}.mid')\n",
    "    \n",
    "    \n",
    "    \n",
    "# test_type = 'Tester'\n",
    "# x_train, x_test, y_train, y_test, notes_dict = get_pickles(f'{test_type}.p')\n",
    "# reverse_notes = {n:idx for idx, n in notes_dict.items()}\n",
    "# first_seq = read_ts_midi('../EDM/All/FLPN Deep House - 04.mid', timestep = 10)\n",
    "# first_seq = [reverse_notes[i] for i in first_seq]\n",
    "# print(first_seq)\n",
    "# model = load_model(f'ModelWeights/LSTM_{test_type}.h5')\n",
    "\n",
    "# new_song = generate_music(model, first_seq, total_runs = 50)\n",
    "# new_song_notes = [notes_dict[i] for i in new_song]\n",
    "    \n",
    "# convert_to_midi(new_song_notes, filename = f'Tests/{test_type}.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WITH INSTRUMENT\n",
    "def read_ts_midi(file_path, timestep): \n",
    "    notes = [] \n",
    "    notes_to_parse = None \n",
    "    instruments = []\n",
    "\n",
    "    midi= converter.parse(file_path)\n",
    "    \n",
    "    s2 = instrument.partitionByInstrument(midi)\n",
    "    for part in s2.parts:\n",
    "        str_part = str(part)\n",
    "        if str_part == '<music21.stream.Part>': \n",
    "            continue \n",
    "        else:\n",
    "            cutoff_len = len('<music21.stream.Part ') \n",
    "            instru = str_part[cutoff_len:]\n",
    "            instru = instru[:instru.find('>')]\n",
    "            notes_to_parse = part.recurse() \n",
    "            for element in notes_to_parse: \n",
    "                if isinstance(element, note.Note): \n",
    "                    notes.append(str(element.pitch))\n",
    "                    instruments.append(instru)\n",
    "                elif isinstance(element, chord.Chord): \n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))      \n",
    "                    instruments.append(instru)\n",
    "                    \n",
    "    return np.array(list(zip(instruments, notes)))\n",
    "        \n",
    "def get_pickles(pick_name): \n",
    "    x_train, x_test, y_train, y_test, notes_dict = pickle.load(open(f'../Pickles/{pick_name}', 'rb'))\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, notes_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_music(model, first_sequence, timestep = 10, total_runs = 25):\n",
    "    song = [i for i in first_sequence]\n",
    "    total_iter = 0\n",
    "    current_idx = 0\n",
    "    while total_iter < total_runs: \n",
    "        current_seq = song[current_idx: timestep+current_idx]\n",
    "        current_seq = np.array(current_seq).reshape(1, timestep, 1)\n",
    "        next_note = np.argmax(model.predict(current_seq)[0])\n",
    "        song.append(next_note)\n",
    "        total_iter += 1 \n",
    "        current_idx += 1\n",
    "    \n",
    "    return song\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test, notes_dict = get_pickles('50cent.p')\n",
    "reverse_notes = {n:idx for idx, n in notes_dict.items()}\n",
    "first_seq = read_ts_midi('../50Cent/50_cent-piggy_bank.mid', timestep = 10)\n",
    "first_seq = [' '.join(i) for i in first_seq]\n",
    "first_seq = [reverse_notes[i] for i in first_seq]\n",
    "model = load_model('ModelWeights/LSTM.h5')\n",
    "\n",
    "new_song = generate_music(model, first_seq)\n",
    "new_song_notes = [notes_dict[i] for i in new_song]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_song_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_midi(prediction_output):\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for ints_note in prediction_output:\n",
    "        ints_note_split = ints_note.split(' ')\n",
    "        pattern = ints_note_split[-1]\n",
    "        instru = ' '.join(ints_note_split[:-1])\n",
    "        print(instru)\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                output_notes.append(instrument.Instrument(instrumentName = instru))\n",
    "                cn=int(current_note)\n",
    "                new_note = note.Note(cn)\n",
    "#                 new_note.storedInstrument = instrument.Violin()\n",
    "                notes.append(new_note)\n",
    "                \n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "            \n",
    "        # pattern is a note\n",
    "        else:\n",
    "            output_notes.append(instrument.Instrument(instrumentName = instru))\n",
    "\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "#             new_note.storedInstrument = instrument.Violin()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += .5\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='music.mid')\n",
    "    \n",
    "convert_to_midi(new_song_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(new_song_notes)\n",
    "print(new_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(x_test)-1)\n",
    "random_music = x_test[idx]\n",
    "predictions = [] \n",
    "for i in range(10): \n",
    "    random_music = random_music.reshape(1,timestep,1)\n",
    "    prob = lstm_model.predict(random_music)[0]\n",
    "    y_pred = np.argmax(prob, axis = 0)\n",
    "    \n",
    "    predictions.append(y_pred)\n",
    "    \n",
    "    random_music = np.insert(random_music[0], len(random_music[0]),y_pred)\n",
    "    random_music = random_music[1:]\n",
    "    \n",
    "print(predictions)\n",
    "\n",
    "unique_x = list(set(x.ravel()))\n",
    "\n",
    "x_int_dict = dict((i,n) for n,i in enumerate(unique_x))\n",
    "\n",
    "predicted_notes = [x_int_dict[i] for i in predictions]\n",
    "predicted_notes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MusicGenerator",
   "language": "python",
   "name": "musicgenerator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
